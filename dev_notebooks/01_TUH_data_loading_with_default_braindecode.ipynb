{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I show that braindecode dataset's create_from_mne_raw does not actually work with any edf file.\n",
    "\n",
    "This is because in their example, the data loaded has annotations, which is necessary for the creation of the database. If there are no annotations in the mne rawEDF object after it is read, either add them yourself or use another function than create_windows_from_events , such as create_fixed_length_windows. This is because events are created using annotations, hence without annotations this function returns an error.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the files, you want a list of rawEDF (TUSZ)\n",
    "## TUSZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 12:25:09.703596: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-15 12:25:09.740423: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-15 12:25:09.740485: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-15 12:25:09.740506: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-15 12:25:09.748103: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-15 12:25:09.748744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-15 12:25:10.995622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/rds/general/user/nm2318/home/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseDataset at 0x1541fd38a290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne            \n",
    "from braindecode.datasets import create_from_mne_raw\n",
    "from braindecode.datasets.base import BaseDataset, BaseConcatDataset\n",
    "import glob\n",
    "import os\n",
    "\n",
    "TUSZ_path = ('/rds/general/user/nm2318/home/projects/scott_data_tuh/live/tuh_eeg_seizure/v2.0.0')\n",
    "\n",
    "debug = True \n",
    "file_paths = glob.glob(os.path.join(TUSZ_path, '**/*.edf'), recursive=True)\n",
    "# If debug is true and the number of edf files is bigger than 5, select \n",
    "# the first 5 file_paths\n",
    "if (debug and len(file_paths)>100):\n",
    "        file_paths = file_paths[0:5]\n",
    "\n",
    "# Load each of the files\n",
    "parts = [mne.io.read_raw_edf(path, preload=True, stim_channel='auto', verbose='WARNING') for path in file_paths]\n",
    "attrs = [part.info for part in parts]\n",
    "\n",
    "base_datasets = [BaseDataset(raw) for raw in parts]\n",
    "base_datasets[0].raw\n",
    "base_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<braindecode.datasets.base.BaseDataset at 0x1541fd38a290>,\n",
       " <braindecode.datasets.base.BaseDataset at 0x1541f82d44d0>,\n",
       " <braindecode.datasets.base.BaseDataset at 0x1541f82d5550>,\n",
       " <braindecode.datasets.base.BaseDataset at 0x1541f82d42d0>,\n",
       " <braindecode.datasets.base.BaseDataset at 0x1541f723fa50>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a concatenated version of the basedataset.\n",
    "What you need to feed the class is a list of all the basedatasets you want to concatenate, even if it only contains 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Annotations | 0 segments>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_concat_datasets = BaseConcatDataset(base_datasets)\n",
    "base_concat_datasets.datasets[0].raw #sanity check\n",
    "base_concat_datasets.datasets[0].raw.annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load them as windows dataset \n",
    "This fails when using tusz data, I added a some checkpoints in the braindecode library file to show what differs between the examples and using the TUSZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1, in windowers.py\n",
      "<braindecode.datasets.base.BaseConcatDataset object at 0x1542ba950d50>\n",
      "\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1542bc140750>\n",
      "<RawEDF | aaaaaexe_s001_t001.edf, 41 x 356500 (1426.0 s), ~111.6 MB, data loaded>\n",
      "<Annotations | 0 segments>\n",
      "[]\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {}\n",
      "checkpoint 5, event, eventid:  [] {}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m windows_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_from_mne_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_start_offset_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_stop_offset_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_stride_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/braindecode/datasets/mne.py:72\u001b[0m, in \u001b[0;36mcreate_from_mne_raw\u001b[0;34m(raws, trial_start_offset_samples, trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window, descriptions, mapping, preload, drop_bad_windows, accepted_bads_ratio)\u001b[0m\n\u001b[1;32m     69\u001b[0m     base_datasets \u001b[38;5;241m=\u001b[39m [BaseDataset(raw) \u001b[38;5;28;01mfor\u001b[39;00m raw \u001b[38;5;129;01min\u001b[39;00m raws]\n\u001b[1;32m     71\u001b[0m base_datasets \u001b[38;5;241m=\u001b[39m BaseConcatDataset(base_datasets)\n\u001b[0;32m---> 72\u001b[0m windows_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_windows_from_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_start_offset_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_start_offset_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_stop_offset_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_stop_offset_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_stride_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_stride_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_bad_windows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_bad_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccepted_bads_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccepted_bads_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m windows_datasets\n",
      "File \u001b[0;32m~/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/braindecode/preprocessing/windowers.py:126\u001b[0m, in \u001b[0;36mcreate_windows_from_events\u001b[0;34m(concat_ds, trial_start_offset_samples, trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window, mapping, preload, drop_bad_windows, picks, reject, flat, on_missing, accepted_bads_ratio, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    123\u001b[0m mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m infer_mapping \u001b[38;5;28;01melse\u001b[39;00m mapping\n\u001b[1;32m    124\u001b[0m infer_window_size_stride \u001b[38;5;241m=\u001b[39m window_size_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m list_of_windows_ds \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_create_windows_from_events\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_window_size_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial_start_offset_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_stop_offset_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_stride_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_bad_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_bads_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcat_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseConcatDataset(list_of_windows_ds)\n",
      "File \u001b[0;32m~/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/EEG_pipeline_V4/lib/python3.11/site-packages/braindecode/preprocessing/windowers.py:284\u001b[0m, in \u001b[0;36m_create_windows_from_events\u001b[0;34m(ds, infer_mapping, infer_window_size_stride, trial_start_offset_samples, trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window, mapping, preload, drop_bad_windows, picks, reject, flat, on_missing, accepted_bads_ratio, verbose)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# XXX This could probably be simplified by using chunk_duration in\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m#     `events_from_annotations`\u001b[39;00m\n\u001b[1;32m    283\u001b[0m last_samp \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mfirst_samp \u001b[38;5;241m+\u001b[39m ds\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mn_times\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstops\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m trial_stop_offset_samples \u001b[38;5;241m>\u001b[39m last_samp:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial_stop_offset_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m too large. Stop of last trial \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstops[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) + \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial_stop_offset_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial_stop_offset_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be smaller than length of\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m recording (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_window_size_stride:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# window size is trial size\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "windows_dataset = create_from_mne_raw(\n",
    "    parts,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    window_size_samples=500,\n",
    "    window_stride_samples=500,\n",
    "    drop_last_window=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: do it for their data - look how it works and how the checkpoints differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default location ~/mne_data for EEGBCI...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "from braindecode.datasets import (create_from_mne_raw, create_from_mne_epochs)\n",
    "from braindecode.datasets.base import BaseDataset, BaseConcatDataset\n",
    "\n",
    "\n",
    "subject_id = 12\n",
    "event_codes = [5, 6, 9, 10, 13, 14]\n",
    "# event_codes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "# This will download the files if you don't have them yet,\n",
    "# and then return the paths to the files.\n",
    "physionet_paths = mne.datasets.eegbci.load_data(subject_id, event_codes, update_path=False)\n",
    "\n",
    "# Load each of the files\n",
    "parts_test = [mne.io.read_raw_edf(path, preload=True, stim_channel='auto', verbose='WARNING') for path in physionet_paths]\n",
    "\n",
    "base_datasets_test = [BaseDataset(raw) for raw in [parts_test]]\n",
    "base_concat_datasets_test = BaseConcatDataset(base_datasets_test)\n",
    "len(parts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1, in windowers.py\n",
      "<braindecode.datasets.base.BaseConcatDataset object at 0x1541ec02eb10>\n",
      "\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02db50>\n",
      "<RawEDF | S012R05.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (7), T2 (8)>\n",
      "['T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2'\n",
      " 'T0' 'T1' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1'\n",
      " 'T0' 'T2']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     1]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     2]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     1]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     2]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     2]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     1]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     2]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     1]\n",
      " [10496     0     0]\n",
      " [11152     0     1]\n",
      " [11808     0     0]\n",
      " [12464     0     2]\n",
      " [13120     0     0]\n",
      " [13776     0     2]\n",
      " [14432     0     0]\n",
      " [15088     0     1]\n",
      " [15744     0     0]\n",
      " [16400     0     2]\n",
      " [17056     0     0]\n",
      " [17712     0     1]\n",
      " [18368     0     0]\n",
      " [19024     0     2]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02fc90>\n",
      "<RawEDF | S012R06.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (7), T2 (8)>\n",
      "['T0' 'T2' 'T0' 'T1' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2'\n",
      " 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T1' 'T0' 'T2'\n",
      " 'T0' 'T2']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     2]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     1]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     1]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     2]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     2]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     1]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     2]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     1]\n",
      " [10496     0     0]\n",
      " [11152     0     2]\n",
      " [11808     0     0]\n",
      " [12464     0     1]\n",
      " [13120     0     0]\n",
      " [13776     0     2]\n",
      " [14432     0     0]\n",
      " [15088     0     1]\n",
      " [15744     0     0]\n",
      " [16400     0     1]\n",
      " [17056     0     0]\n",
      " [17712     0     2]\n",
      " [18368     0     0]\n",
      " [19024     0     2]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02fad0>\n",
      "<RawEDF | S012R09.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (8), T2 (7)>\n",
      "['T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1'\n",
      " 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T1' 'T0' 'T2'\n",
      " 'T0' 'T1']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     1]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     2]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     1]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     2]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     1]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     2]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     1]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     2]\n",
      " [10496     0     0]\n",
      " [11152     0     1]\n",
      " [11808     0     0]\n",
      " [12464     0     2]\n",
      " [13120     0     0]\n",
      " [13776     0     2]\n",
      " [14432     0     0]\n",
      " [15088     0     1]\n",
      " [15744     0     0]\n",
      " [16400     0     1]\n",
      " [17056     0     0]\n",
      " [17712     0     2]\n",
      " [18368     0     0]\n",
      " [19024     0     1]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02f750>\n",
      "<RawEDF | S012R10.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (7), T2 (8)>\n",
      "['T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T1'\n",
      " 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1'\n",
      " 'T0' 'T2']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     2]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     1]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     2]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     1]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     2]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     1]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     1]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     2]\n",
      " [10496     0     0]\n",
      " [11152     0     2]\n",
      " [11808     0     0]\n",
      " [12464     0     1]\n",
      " [13120     0     0]\n",
      " [13776     0     2]\n",
      " [14432     0     0]\n",
      " [15088     0     1]\n",
      " [15744     0     0]\n",
      " [16400     0     2]\n",
      " [17056     0     0]\n",
      " [17712     0     1]\n",
      " [18368     0     0]\n",
      " [19024     0     2]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02ded0>\n",
      "<RawEDF | S012R13.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (8), T2 (7)>\n",
      "['T0' 'T2' 'T0' 'T1' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2'\n",
      " 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1'\n",
      " 'T0' 'T1']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     2]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     1]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     1]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     2]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     1]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     2]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     2]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     1]\n",
      " [10496     0     0]\n",
      " [11152     0     2]\n",
      " [11808     0     0]\n",
      " [12464     0     1]\n",
      " [13120     0     0]\n",
      " [13776     0     1]\n",
      " [14432     0     0]\n",
      " [15088     0     2]\n",
      " [15744     0     0]\n",
      " [16400     0     2]\n",
      " [17056     0     0]\n",
      " [17712     0     1]\n",
      " [18368     0     0]\n",
      " [19024     0     1]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n",
      "checkpoint 2, in windowers.py, _create_... function\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ec02e110>\n",
      "<RawEDF | S012R14.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>\n",
      "<Annotations | 30 segments: T0 (15), T1 (8), T2 (7)>\n",
      "['T0' 'T1' 'T0' 'T2' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T1'\n",
      " 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2' 'T0' 'T1' 'T0' 'T2'\n",
      " 'T0' 'T1']\n",
      "checkpoint 3, infer_mapping is True\n",
      "checkpoint 4, mapping:  {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
      "checkpoint 5, event, eventid:  [[    0     0     0]\n",
      " [  656     0     1]\n",
      " [ 1312     0     0]\n",
      " [ 1968     0     2]\n",
      " [ 2624     0     0]\n",
      " [ 3280     0     2]\n",
      " [ 3936     0     0]\n",
      " [ 4592     0     1]\n",
      " [ 5248     0     0]\n",
      " [ 5904     0     2]\n",
      " [ 6560     0     0]\n",
      " [ 7216     0     1]\n",
      " [ 7872     0     0]\n",
      " [ 8528     0     1]\n",
      " [ 9184     0     0]\n",
      " [ 9840     0     2]\n",
      " [10496     0     0]\n",
      " [11152     0     1]\n",
      " [11808     0     0]\n",
      " [12464     0     2]\n",
      " [13120     0     0]\n",
      " [13776     0     1]\n",
      " [14432     0     0]\n",
      " [15088     0     2]\n",
      " [15744     0     0]\n",
      " [16400     0     1]\n",
      " [17056     0     0]\n",
      " [17712     0     2]\n",
      " [18368     0     0]\n",
      " [19024     0     1]] {'T0': 0, 'T1': 1, 'T2': 2}\n",
      "Using data from preloaded Raw for 30 events and 500 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "windows_dataset_test = create_from_mne_raw(\n",
    "    parts_test,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    window_size_samples=500,\n",
    "    window_stride_samples=500,\n",
    "    drop_last_window=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<braindecode.datasets.base.BaseConcatDataset object at 0x1541ebf0e4d0>\n",
      "<braindecode.datasets.base.BaseDataset object at 0x1541ebebc290>\n",
      "[<RawEDF | S012R05.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>, <RawEDF | S012R06.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>, <RawEDF | S012R09.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>, <RawEDF | S012R10.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>, <RawEDF | S012R13.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>, <RawEDF | S012R14.edf, 64 x 19680 (123.0 s), ~9.7 MB, data loaded>]\n"
     ]
    }
   ],
   "source": [
    "print(base_concat_datasets_test)\n",
    "print(base_concat_datasets_test.datasets[0])\n",
    "print(base_concat_datasets_test.datasets[0].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Annotations | 30 segments: T0 (15), T1 (7), T2 (8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_test[0].annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T0', 'T1', 'T0', 'T2', 'T0', 'T1', 'T0', 'T2', 'T0', 'T2', 'T0',\n",
       "       'T1', 'T0', 'T2', 'T0', 'T1', 'T0', 'T1', 'T0', 'T2', 'T0', 'T2',\n",
       "       'T0', 'T1', 'T0', 'T2', 'T0', 'T1', 'T0', 'T2'], dtype='<U2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_test[0].annotations.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG_pipeline_V4",
   "language": "python",
   "name": "eeg_pipeline_v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
